{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e0463ae",
   "metadata": {},
   "source": [
    "# üß† Perceptron: Summary\n",
    "\n",
    "## What is a Perceptron?\n",
    "\n",
    "A **Perceptron** is the simplest type of neural network ‚Äî basically a single artificial neuron used for **binary classification** (two-class problems). It tries to find a straight line (or hyperplane) to separate two groups.\n",
    "\n",
    "Invented in **1957 by Frank Rosenblatt**, it is a **linear classifier**.\n",
    "\n",
    "---\n",
    "\n",
    "## How It Works\n",
    "\n",
    "* **Inputs (x):** Features like x‚ÇÅ, x‚ÇÇ, ..., x‚Çô.\n",
    "* **Weights (w):** Each input has a weight showing importance.\n",
    "* **Net Input:**\n",
    "\n",
    "  * Formula: **z = w ¬∑ x + b**\n",
    "* **Activation Function (Step function):**\n",
    "\n",
    "  * If **z > 0 ‚Üí output = 1**\n",
    "  * If **z ‚â§ 0 ‚Üí output = -1**\n",
    "\n",
    "The **decision boundary** is where **z = 0**.\n",
    "\n",
    "---\n",
    "\n",
    "## Learning: The Perceptron Algorithm\n",
    "\n",
    "The perceptron learns by correcting its mistakes.\n",
    "\n",
    "### Steps:\n",
    "\n",
    "1. Initialize weights and bias (0 or small random values).\n",
    "2. For each training example:\n",
    "\n",
    "   * **Predict** output.\n",
    "   * **Compare** prediction with true label.\n",
    "   * **Update** only if wrong.\n",
    "\n",
    "### Update Rule:\n",
    "\n",
    "* **w_new = w_old + Œ∑ * y * x**\n",
    "* **b_new = b_old + Œ∑ * y**\n",
    "\n",
    "Where:\n",
    "\n",
    "* **Œ∑** = learning rate\n",
    "* **y** = true label (1 or -1)\n",
    "\n",
    "---\n",
    "\n",
    "## Why Updates Work\n",
    "\n",
    "* If **false negative** (y = 1 but predicted -1):\n",
    "\n",
    "  * Add Œ∑x ‚Üí makes output more positive.\n",
    "* If **false positive** (y = -1 but predicted 1):\n",
    "\n",
    "  * Subtract Œ∑x ‚Üí makes output more negative.\n",
    "\n",
    "---\n",
    "\n",
    "## Example Problem: OR Gate\n",
    "\n",
    "The OR gate is linearly separable, so the perceptron can solve it.\n",
    "\n",
    "* (0, 0) ‚Üí -1\n",
    "* (0, 1) ‚Üí 1\n",
    "* (1, 0) ‚Üí 1\n",
    "* (1, 1) ‚Üí 1\n",
    "\n",
    "Python code (not included here) trains the perceptron to correctly classify these points.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c726c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron training complete.\n",
      "Learned Weights: [0.1 0.1]\n",
      "Learned Bias: -0.1\n",
      "\n",
      "--- Predictions ---\n",
      "Input [0, 0] -> Output: -1\n",
      "Input [0, 1] -> Output: 1\n",
      "Input [1, 0] -> Output: 1\n",
      "Input [1, 1] -> Output: 1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Perceptron:\n",
    "    \"\"\"\n",
    "    A simple Perceptron classifier.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    learning_rate : float\n",
    "        The step size for weight updates (default 0.01).\n",
    "    n_iters : int\n",
    "        Number of passes over the training dataset (epochs) (default 100).\n",
    "        \n",
    "    Attributes\n",
    "    ----------\n",
    "    weights : 1d-array\n",
    "        Weights after fitting.\n",
    "    bias : scalar\n",
    "        Bias unit after fitting.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, learning_rate=0.01, n_iters=100):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iters = n_iters\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def _activation_function(self, x):\n",
    "        \"\"\"The Heaviside step function.\"\"\"\n",
    "        # Returns 1 if x >= 0, otherwise -1\n",
    "        return np.where(x >= 0, 1, -1)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit training data.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like}, shape = [n_samples, n_features]\n",
    "            Training vectors.\n",
    "        y : array-like, shape = [n_samples]\n",
    "            Target values (must be 1 or -1).\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # 1. Initialize weights and bias\n",
    "        # We initialize to zeros for simplicity\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "        \n",
    "        # We use 1 and -1 for our class labels\n",
    "        y_ = np.array([1 if i > 0 else -1 for i in y])\n",
    "\n",
    "        # 2. Loop for n_iters (epochs)\n",
    "        for _ in range(self.n_iters):\n",
    "            # 3. Loop over each training sample\n",
    "            for idx, x_i in enumerate(X):\n",
    "                # Calculate the net input (w*x + b)\n",
    "                linear_output = np.dot(x_i, self.weights) + self.bias\n",
    "                \n",
    "                # Make a prediction\n",
    "                y_predicted = self._activation_function(linear_output)\n",
    "                \n",
    "                # 4. Compare and Update (if wrong)\n",
    "                if y_[idx] != y_predicted:\n",
    "                    # Apply the Perceptron update rule\n",
    "                    update = self.learning_rate * y_[idx]\n",
    "                    self.weights += update * x_i\n",
    "                    self.bias += update\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict class labels for new data.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like}, shape = [n_samples, n_features]\n",
    "            Input vectors.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        y_pred : array, shape = [n_samples]\n",
    "            Predicted class labels (1 or -1).\n",
    "        \"\"\"\n",
    "        linear_output = np.dot(X, self.weights) + self.bias\n",
    "        return self._activation_function(linear_output)\n",
    "\n",
    "# --- Main execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # We will test the Perceptron on the OR gate\n",
    "    # We use 1 and -1 as our labels for True and False\n",
    "    \n",
    "    # Input data (OR gate)\n",
    "    X = np.array([\n",
    "        [0, 0],\n",
    "        [0, 1],\n",
    "        [1, 0],\n",
    "        [1, 1]\n",
    "    ])\n",
    "    \n",
    "    # Target labels (OR gate results)\n",
    "    # 0 or 0 = 0 (False -> -1)\n",
    "    # 0 or 1 = 1 (True -> 1)\n",
    "    # 1 or 0 = 1 (True -> 1)\n",
    "    # 1 or 1 = 1 (True -> 1)\n",
    "    y = np.array([-1, 1, 1, 1])\n",
    "\n",
    "    # Create and train the perceptron\n",
    "    p = Perceptron(learning_rate=0.1, n_iters=10)\n",
    "    p.fit(X, y)\n",
    "    \n",
    "    print(\"Perceptron training complete.\")\n",
    "    print(f\"Learned Weights: {p.weights}\")\n",
    "    print(f\"Learned Bias: {p.bias}\")\n",
    "    \n",
    "    # Test the predictions\n",
    "    print(\"\\n--- Predictions ---\")\n",
    "    test_00 = p.predict([0, 0])\n",
    "    test_01 = p.predict([0, 1])\n",
    "    test_10 = p.predict([1, 0])\n",
    "    test_11 = p.predict([1, 1])\n",
    "    \n",
    "    print(f\"Input [0, 0] -> Output: {test_00}\")\n",
    "    print(f\"Input [0, 1] -> Output: {test_01}\")\n",
    "    print(f\"Input [1, 0] -> Output: {test_10}\")\n",
    "    print(f\"Input [1, 1] -> Output: {test_11}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc1a00b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## ‚ö†Ô∏è Key Concepts and Limitations\n",
    "\n",
    "**Convergence:** The Perceptron Convergence Theorem guarantees the algorithm will find a separating line *only if* the data is linearly separable. If not, it never converges (hence setting a max number of iterations).\n",
    "\n",
    "**Linear Separability (XOR Problem):** The perceptron's major limitation is that it can only solve linearly separable tasks.\n",
    "\n",
    "**XOR Example (not linearly separable):**\n",
    "\n",
    "* (0, 0) ‚Üí 0\n",
    "* (0, 1) ‚Üí 1\n",
    "* (1, 0) ‚Üí 1\n",
    "* (1, 1) ‚Üí 0\n",
    "\n",
    "No single straight line can separate the outputs. This led to the discovery that single-layer perceptrons are limited, contributing to the \"AI Winter.\" The solution became **Multi-Layer Perceptrons**, which handle nonlinear problems using multiple stacked layers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b573421d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
